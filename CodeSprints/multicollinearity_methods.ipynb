{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "204733d1",
   "metadata": {},
   "source": [
    "## Multicollinearity and Regression Analysis\n",
    "In this tutorial, we will be using a spatial dataset of county-level election and demographic statistics for the United States. This time, we'll explore different methods to diagnose and account for multicollinearity in our data. Specifically, we'll calculate variance inflation factor (VIF), and compare parameter estimates and model fit in a multivariate regression predicting 2016 county voting preferences using an OLS model, a ridge regression, a lasso regression, and an elastic net regression.\n",
    "\n",
    "Objectives:\n",
    "* ***Calculate a variance inflation factor to diagnose multicollinearity.***\n",
    "* ***Use geographicall weighted regression to identify if the multicollinearity is scale dependent.***\n",
    "* ***Interpret model summary statistics.***\n",
    "* ***Describe how multicollinearity impacts stability in parameter esimates.***\n",
    "* ***Explain the variance/bias tradeoff and describe how to use it to improve models***\n",
    "* ***Draw a conclusion based on contrasting models.***\n",
    "\n",
    "Review: \n",
    "* [Dormann, C. et al. (2013). Collinearity: a review of methods to deal with it and a simulation study evaluating their performance. Ecography, 36(1), 27-46.](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1600-0587.2012.07348.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf668f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from libpysal.weights.contiguity import Queen\n",
    "import libpysal\n",
    "from statsmodels.api import OLS\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec4204",
   "metadata": {},
   "source": [
    "First, we're going to load the 'Elections' dataset from the libpysal library, which is a very easy to use API that accesses the Geodata Center at the University of Chicago.\n",
    "\n",
    "* More on spatial data science resources from UC: https://spatial.uchicago.edu/\n",
    "* A list of datasets available through lipysal: https://geodacenter.github.io/data-and-lab//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de38c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libpysal.examples import load_example\n",
    "elections = load_example('Elections')\n",
    "#note the folder where your data now lives:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fe6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let's see what files are available in the 'Elections' data example\n",
    "elections.get_file_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c29a3",
   "metadata": {},
   "source": [
    "When you are out in the world doing research, you often will not find a ready-made function to download your data. That's okay! You know how to get this dataset without using pysal! Do a quick internal review of online data formats and automatic data downloads.\n",
    "\n",
    "### TASK 1: Use urllib functions to download this file directly from the internet to you H:/EnvDatSci folder (not your git repository). Extract the zipped file you've downloaded into a subfolder called H:/EnvDatSci/elections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 code here:\n",
    "#import required function:\n",
    "\n",
    "\n",
    "#define online filepath (aka url):\n",
    "\n",
    "\n",
    "#define local filepath:\n",
    "\n",
    "\n",
    "#download elections data:\n",
    "\n",
    "\n",
    "#unzip file: see if google can help you figure this one out!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea133475",
   "metadata": {},
   "source": [
    "### TASK 2: Use geopandas to read in this shapefile. Call your geopandas.DataFrame \"votes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81455057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2: Use geopandas to read in this shapefile. Call your geopandas.DataFrame \"votes\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c603c7b4",
   "metadata": {},
   "source": [
    "### EXTRA CREDIT TASK (+2pts): use os to delete the elections data downloaded by pysal in your C: drive that you are no longer using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d8bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra credit task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae67e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's view the shapefile to get a general idea of the geometry we're looking at:\n",
    "%matplotlib inline\n",
    "votes.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the first few line]s of the dataset\n",
    "votes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ad89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since there are too many columns for us to view on a signle page using \"head\", we can just print out the column names so we have them all listed for reference\n",
    "for col in votes.columns: \n",
    "    print(col) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa277c42",
   "metadata": {},
   "source": [
    "#### You can use pandas summary statistics to get an idea of how county-level data varies across the United States. \n",
    "### TASK 3: For example, how did the county mean percent Democratic vote change between 2012 (pct_dem_12) and 2016 (pct_dem_16)?\n",
    "\n",
    "Look here for more info on pandas summary statistics:https://www.earthdatascience.org/courses/intro-to-earth-data-science/scientific-data-structures-python/pandas-dataframes/run-calculations-summary-statistics-pandas-dataframes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818f70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f15a0",
   "metadata": {},
   "source": [
    "We can also plot histograms of the data. Below, smoothed histograms from the seaborn package (imported as sns) let us get an idea of the distribution of percent democratic votes in 2012 (left) and 2016 (right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57df83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms:\n",
    "f,ax = plt.subplots(1,2, figsize=(2*3*1.6, 2))\n",
    "for i,col in enumerate(['pct_dem_12','pct_dem_16']):\n",
    "    sns.kdeplot(votes[col].values, shade=True, color='slategrey', ax=ax[i])\n",
    "    ax[i].set_title(col.split('_')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c314d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot spatial distribution of # dem vote in 2012 and 2016 with histogram.\n",
    "f,ax = plt.subplots(2,2, figsize=(1.6*6 + 1,2.4*3), gridspec_kw=dict(width_ratios=(6,1)))\n",
    "for i,col in enumerate(['pct_dem_12','pct_dem_16']):\n",
    "    votes.plot(col, linewidth=.05, cmap='RdBu', ax=ax[i,0])\n",
    "    ax[i,0].set_title(['2012','2016'][i] + \"% democratic vote\")\n",
    "    ax[i,0].set_xticklabels('')\n",
    "    ax[i,0].set_yticklabels('')\n",
    "    sns.kdeplot(votes[col].values, ax=ax[i,1], vertical=True, shade=True, color='slategrey')\n",
    "    ax[i,1].set_xticklabels('')\n",
    "    ax[i,1].set_ylim(-1,1)\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6ec1f",
   "metadata": {},
   "source": [
    "### TASK 4: Make a new column on your geopandas dataframe called \"pct_dem_change\" and plot it using the syntax above. Explain the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ce3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: add new column pct_dem_change to votes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c7a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 4: plot your pct_dem_change variable on a map:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946cb5c6",
   "metadata": {},
   "source": [
    "Click on this url to learn more about the variables in this dataset: https://geodacenter.github.io/data-and-lab//county_election_2012_2016-variables/\n",
    "As you can see, there are a lot of data values available in this dataset. Let's say we want to learn more about what county-level factors influence percent change in democratic vote between (pct_dem_change).\n",
    "\n",
    "Looking at the data description on the link above, you see that this is an exceptionally large dataset with many variables. During lecture, we discussed how there are two types of multicollinearity in our data:\n",
    "\n",
    "* *Intrinsic multicollinearity:* is an artifact of how we make observations. Often our measurements serve as proxies for some latent process (for example, we can measure percent silt, percent sand, and percent clay as proxies for the latent variable of soil texture). There will be slight variability in the information content between each proxy measurement, but they will not be independent of one another.\n",
    "\n",
    "* *Incidental collinearity:* is an artifact of how we sample complex populations. If we collect data from a subsample of the landscape where we don't see all combinations of our predictor variables (do not have good cross replication across our variables). We often induce collinearity in our data just because we are limitted in our ability to sample the environment at the scale of temporal/spatial variability of our process of interest. Incidental collinearity is a model formulation problem.(See here for more info on how to avoid it: https://people.umass.edu/sdestef/NRC%20601/StudyDesignConcepts.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b59ac",
   "metadata": {},
   "source": [
    "### TASK 5: Looking at the data description, pick two variables that you believe will be intrinsically multicollinear. List and describe these variables. Why do you think they will be collinear? Is this an example of *intrinsic* or *incidental* collinearity?\n",
    "\n",
    "*Click on this box to enter text*\n",
    "I chose:   \n",
    "* \"RHI125214\", #White alone, percent, 2014\n",
    "* \"RHI225214\", #Black or African American alone, percent, 2014\n",
    "These variables are intrinsically multicollinear. A decrease in one of a finite number of races implicitly signifies an increase in another race."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18207d3e",
   "metadata": {},
   "source": [
    "## Multivariate regression in observational data:\n",
    "Our next step is to formulate our predictive/diagnostic model. We want to create a subset of the \"votes\" geopandas data frame that contains ten predictor variables and our response variable (pct_pt_16) two variables you selected under TASK 1. First, create a list of the variables you'd like to select.\n",
    "\n",
    "### TASK 6: Create a subset of votes called \"my_list\" containing only your selected predictor variables. Make sure you use the two variables selected under TASK 3, and eight additional variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d4c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: create a subset of votes called \"my list\" with all your subset variables.\n",
    "#my_list = [\"pct_pt_16\", <list your variables here>]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1835a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to make sure all your columns are there:\n",
    "votes[my_list].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d9dc4",
   "metadata": {},
   "source": [
    "### Scatterplot matrix\n",
    "We call the process of getting to know your data (ranges and distributions of the data, as well as any relationships between variables) \"exploratory data analysis\". Pairwise plots of your variables, called scatterplots, can provide a lot of insight into the type of relationships you have between variables. A scatterplot matrix is a pairwise comparison of all variables in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f67ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use seaborn.pairplot to plot a scatterplot matrix of you 10 variable subset:\n",
    "sns.pairplot(votes[my_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b1b309",
   "metadata": {},
   "source": [
    "### TASK 7: Do you observe any collinearity in this dataset? How would you describe the relationship between your two \"incidentally collinear\" variables that you selected based on looking at variable descriptions? \n",
    "\n",
    "*Type answer here*\n",
    "\n",
    "\n",
    "### TASK 8: What is plotted on the diagonal panels of the scatterplot matrix?\n",
    "\n",
    "*Type answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b42b91",
   "metadata": {},
   "source": [
    "## Diagnosing collinearity globally:\n",
    "During class, we discussed the Variance Inflation Factor, which describes the magnitude of variance inflation that can be expected in an OLS parameter estimate for a given variable *given pairwise collinearity between that variable and another variable*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be64d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIF = 1/(1-R2) of a pairwise OLS regression between two predictor variables\n",
    "#We can use a built-in function \"variance_inflation_factor\" from statsmodel.api to calculate VIF\n",
    "#Learn more about the function\n",
    "?variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f51dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate VIFs on our dataset\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(votes[my_list[1:10]].values, i) for i in range(votes[my_list[1:10]].shape[1])]\n",
    "vif[\"features\"] = votes[my_list[1:10]].columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d465f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae322791",
   "metadata": {},
   "source": [
    "### Collinearity is always present in observational data. When is it a problem?\n",
    "Generally speaking, VIF > 10 are considered \"too much\" collinearity. But this value is somewhat arbitrary: the extent to which variance inflation will impact your analysis is highly context dependent. There are two primary contexts where variance inflation is problematic:\n",
    "\n",
    " 1\\. **You are using your analysis to evaluate variable importance:** If you are using parameter estimates from your model to diagnose which observations have physically important relationships with your response variable, variance inflation can make an important predictor look unimportant, and parameter estimates will be highly leveraged by small changes in the data. \n",
    "\n",
    " 2\\. **You want to use your model to make predictions in a situation where the specific structure of collinearity between variables may have shifted:** When training a model on collinear data, the model only applies to data with that exact structure of collinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a7a711",
   "metadata": {},
   "source": [
    "### Caluculate a linear regression on the global data:\n",
    "In this next step, we're going to calculate a linear regression in our data an determine whether there is a statistically significant relationship between per capita income and percent change in democratic vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515911da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, forumalate the model. See weather_trend.py in \"Git_101\" for a refresher on how.\n",
    "\n",
    "#extract variable that you want to use to \"predict\"\n",
    "X = np.array(votes[my_list[1:10]].values)\n",
    "#standardize data to assist in interpretation of coefficients\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "#extract variable that we want to \"predict\"\n",
    "Y = np.array(votes['pct_dem_change'].values)\n",
    "#standardize data to assist in interpretation of coefficients\n",
    "Y = (Y - np.mean(X)) / np.std(Y)\n",
    "\n",
    "lm = OLS(Y,X)\n",
    "lm_results = OLS(Y,X).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a265a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d760fd73",
   "metadata": {},
   "source": [
    "### TASK 9: Answer: which coefficients indicate a statisticall significant relationship between parameter and pct_dem_change? What is your most important predictor variable? How can you tell?\n",
    "\n",
    "*Type answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1202f4f3",
   "metadata": {},
   "source": [
    "### TASK10:  Are any of these parameters subject to variance inflation? How can you tell?\n",
    "\n",
    "*Type answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a9235",
   "metadata": {},
   "source": [
    "Now, let's plot our residuals to see if there are any spatial patterns in them.\n",
    "\n",
    "Remember residuals = predicted - fitted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0098b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add model residuals to our \"votes\" geopandas dataframe:\n",
    "votes['lm_resid']=OLS(Y,X).fit().resid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(votes['lm_resid'].values, shade=True, color='slategrey')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc83f6a",
   "metadata": {},
   "source": [
    "### TASK 11: Are our residuals normally distributed with a mean of zero? What does that mean?\n",
    "\n",
    "*Type answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f59dce",
   "metadata": {},
   "source": [
    "## Penalized regression: ridge penalty\n",
    "In penalized regression, we intentionally bias the parameter estimates to stabilize them given collinearity in the dataset.\n",
    "\n",
    "From https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/\n",
    "\"As mentioned before, ridge regression performs ‘L2 regularization‘, i.e. it adds a factor of sum of squares of coefficients in the optimization objective. Thus, ridge regression optimizes the following:\n",
    "\n",
    "**Objective = RSS + α * (sum of square of coefficients)**\n",
    "\n",
    "Here, α (alpha) is the parameter which balances the amount of emphasis given to minimizing RSS vs minimizing sum of square of coefficients. α can take various values:\n",
    "\n",
    "* **α = 0:** The objective becomes same as simple linear regression. We’ll get the same coefficients as simple linear regression.\n",
    "\n",
    "* **α = ∞:** The coefficients will approach zero. Why? Because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite.\n",
    "\n",
    "* **0 < α < ∞:** The magnitude of α will decide the weightage given to different parts of objective. The coefficients will be somewhere between 0 and ones for simple linear regression.\"\n",
    "\n",
    "In other words, the ridge penalty shrinks coefficients such that collinear coefficients will have more similar coefficient values. It has a \"grouping\" tendency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebcc4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when L2=0, Ridge equals OLS\n",
    "model = Ridge(alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "#force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68402c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y)\n",
    "#Print out the model coefficients\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d94d1f",
   "metadata": {},
   "source": [
    "## Penalized regression: lasso penalty\n",
    "\n",
    "From https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/\n",
    "\"LASSO stands for Least Absolute Shrinkage and Selection Operator. I know it doesn’t give much of an idea but there are 2 key words here – ‘absolute‘ and ‘selection‘.\n",
    "\n",
    "Lets consider the former first and worry about the latter later.\n",
    "\n",
    "Lasso regression performs L1 regularization, i.e. it adds a factor of sum of absolute value of coefficients in the optimization objective. Thus, lasso regression optimizes the following:\n",
    "\n",
    "**Objective = RSS + α * (sum of absolute value of coefficients)**\n",
    "Here, α (alpha) works similar to that of ridge and provides a trade-off between balancing RSS and magnitude of coefficients. Like that of ridge, α can take various values. Lets iterate it here briefly:\n",
    "\n",
    "* **α = 0:** Same coefficients as simple linear regression\n",
    "* **α = ∞:** All coefficients zero (same logic as before)\n",
    "* **0 < α < ∞:** coefficients between 0 and that of simple linear regression\n",
    "\n",
    "Yes its appearing to be very similar to Ridge till now. But just hang on with me and you’ll know the difference by the time we finish.\"\n",
    "\n",
    "In other words, the lasso penalty shrinks unimportant coefficients down towards zero, automatically \"selecting\" important predictor variables. But what if that shrunken coefficient is induced by incidental collinearity (i.e. is a feature of how we sampled our data)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f683d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when L1=0, Lasso equals OLS\n",
    "model = Lasso(alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eccdbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "#force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y)\n",
    "#Print out the model coefficients\n",
    "print(model.coef_)\n",
    "#How do these compare with OLS coefficients above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when L1 approaches infinity, certain coefficients will become exactly zero, and MAE equals the variance of our response variable:\n",
    "model = Lasso(alpha=10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd284dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "#force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2dd989",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y)\n",
    "#Print out the model coefficients\n",
    "print(model.coef_)\n",
    "#How do these compare with OLS coefficients above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe1b7ff",
   "metadata": {},
   "source": [
    "### Penalized regression: elastic net penalty\n",
    "\n",
    "In other words, the lasso penalty shrinks unimportant coefficients down towards zero, automatically \"selecting\" important predictor variables. The ridge penalty shrinks coefficients of collinear predictor variables nearer to each other, effectively partitioning the magnitude of response from the response variable between them, instead of \"arbitrarily\" partitioning it to one group.\n",
    "\n",
    "We can also run a regression with a linear combination of ridge and lasso, called the elastic net, that has a cool property called \"group selection.\"\n",
    "\n",
    "The ridge penalty still works to distribute response variance equally between members of \"groups\" of collinear predictor variables. The lasso penalty still works to shrink certain coefficients to exactly zero so they can be ignored in model formulation. The elastic net produces models that are both sparse and stable under collinearity, by shrinking parameters of members of unimportant collinear predictor variables to exactly zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6967ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when L1 approaches infinity, certain coefficients will become exactly zero, and MAE equals the variance of our response variable:\n",
    "model = ElasticNet(alpha=1, l1_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "#force scores to be positive\n",
    "scores = absolute(scores)\n",
    "print('Mean MAE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y)\n",
    "#Print out the model coefficients\n",
    "print(model.coef_)\n",
    "#How do these compare with OLS coefficients above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a540d",
   "metadata": {},
   "source": [
    "### TASK 11: Match these elastic net coefficients up with your original data. Do you see a logical grouping(s) between variables that have non-zero coefficients?Explain why or why not.\n",
    "*Type answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c12c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 11 scratch cell:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geostats_env",
   "language": "python",
   "name": "geostats_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
